{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "spylon-kernel",
   "display_name": "spylon-kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "import org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.sql.SparkSession\nimport org.apache.log4j._\n"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "// Logistic Regression Example\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "// Optional: Use the following code below to set the Error reporting\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- PassengerId: integer (nullable = true)\n |-- Survived: integer (nullable = true)\n |-- Pclass: integer (nullable = true)\n |-- Name: string (nullable = true)\n |-- Sex: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- SibSp: integer (nullable = true)\n |-- Parch: integer (nullable = true)\n |-- Ticket: string (nullable = true)\n |-- Fare: double (nullable = true)\n |-- Cabin: string (nullable = true)\n |-- Embarked: string (nullable = true)\n\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@240e4d4a\ndata: org.apache.spark.sql.DataFrame = [PassengerId: int, Survived: int ... 10 more fields]\n"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "// Spark Session\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "\n",
    "// Use Spark to read in the Titanic csv file.\n",
    "val data = spark.read\n",
    "    .option(\"header\",\"true\")\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "    .format(\"csv\")\n",
    "    .load(\"titanic.csv\")\n",
    "\n",
    "// Print the Schema of the DataFrame\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\nExample Data Row\nSurvived\n0\n\n\nPclass\n3\n\n\nName\nBraund, Mr. Owen Harris\n\n\nSex\nmale\n\n\nAge\n22.0\n\n\nSibSp\n1\n\n\nParch\n0\n\n\nTicket\nA/5 21171\n\n\nFare\n7.25\n\n\nCabin\nnull\n\n\nEmbarked\nS\n\n\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "colnames: Array[String] = Array(PassengerId, Survived, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked)\nfirstrow: org.apache.spark.sql.Row = [1,0,3,Braund, Mr. Owen Harris,male,22.0,1,0,A/5 21171,7.25,null,S]\n"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "\n",
    "// Display Data\n",
    "\n",
    "val colnames = data.columns\n",
    "val firstrow = data.head(1)(0)\n",
    "println(\"\\n\")\n",
    "println(\"Example Data Row\")\n",
    "for(ind <- Range(1,colnames.length)){\n",
    "  println(colnames(ind))\n",
    "  println(firstrow(ind))\n",
    "  println(\"\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "logregdataall: org.apache.spark.sql.DataFrame = [label: int, Pclass: int ... 6 more fields]\nlogregdata: org.apache.spark.sql.DataFrame = [label: int, Pclass: int ... 6 more fields]\n"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "// Setting Up DataFrame for Machine Learning\n",
    "\n",
    "// Grab only the columns we want\n",
    "val logregdataall = data\n",
    "    .select(\n",
    "    data(\"Survived\").as(\"label\"),\n",
    "    $\"Pclass\", $\"Sex\", $\"Age\", $\"SibSp\", $\"Parch\", $\"Fare\", $\"Embarked\")\n",
    "val logregdata = logregdataall.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, VectorIndexer, OneHotEncoder}\nimport org.apache.spark.ml.linalg.Vectors\n"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "// A few things we need to do before Spark can accept the data!\n",
    "// We need to deal with the Categorical columns\n",
    "\n",
    "// Import VectorAssembler and Vectors\n",
    "import org.apache.spark.ml.feature.{VectorAssembler,StringIndexer,VectorIndexer,OneHotEncoder}\n",
    "import org.apache.spark.ml.linalg.Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "genderIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_5df031bcf5a0\nembarkIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_bd926fb2d4b1\ngenderEncoder: org.apache.spark.ml.feature.OneHotEncoder = oneHotEncoder_592439a87496\nembarkEncoder: org.apache.spark.ml.feature.OneHotEncoder = oneHotEncoder_90b5abcee24b\n"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "// Deal with Categorical Columns\n",
    "\n",
    "// 1 Covert strings to numerical values\n",
    "val genderIndexer = new StringIndexer()\n",
    "    .setInputCol(\"Sex\")\n",
    "    .setOutputCol(\"SexIndex\")\n",
    "val embarkIndexer = new StringIndexer()\n",
    "    .setInputCol(\"Embarked\")\n",
    "    .setOutputCol(\"EmbarkIndex\")\n",
    "\n",
    "// 2 Apply one-hot encoder\n",
    "val genderEncoder = new OneHotEncoder()\n",
    "    .setInputCol(\"SexIndex\")\n",
    "    .setOutputCol(\"SexVec\")\n",
    "val embarkEncoder = new OneHotEncoder()\n",
    "    .setInputCol(\"EmbarkIndex\")\n",
    "    .setOutputCol(\"EmbarkVec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_329c572f1324, handleInvalid=error, numInputCols=7\n"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "// Assemble everything together to be (\"label\",\"features\") format\n",
    "val assembler = (new VectorAssembler()\n",
    "    .setInputCols(\n",
    "        Array(\"Pclass\", \"SexVec\", \"Age\",\"SibSp\",\"Parch\",\"Fare\",\"EmbarkVec\"))\n",
    "    .setOutputCol(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, Pclass: int ... 6 more fields]\ntest: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, Pclass: int ... 6 more fields]\n"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "// Split the data into training and test sets\n",
    "val Array(training, test) = logregdata\n",
    "    .randomSplit(Array(0.7, 0.3), seed = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "import org.apache.spark.ml.Pipeline\nlr: org.apache.spark.ml.classification.LogisticRegression = logreg_cf8a94339527\npipeline: org.apache.spark.ml.Pipeline = pipeline_80bda54bc50a\nmodel: org.apache.spark.ml.PipelineModel = pipeline_80bda54bc50a\nresults: org.apache.spark.sql.DataFrame = [label: int, Pclass: int ... 14 more fields]\n"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "// Set up the pipeline\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "val lr = new LogisticRegression()\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "    .setStages(Array(\n",
    "        genderIndexer,embarkIndexer,genderEncoder,embarkEncoder,assembler, lr))\n",
    "\n",
    "// Fit the pipeline to training documents.\n",
    "val model = pipeline.fit(training)\n",
    "\n",
    "// Get Results on Test Set\n",
    "val results = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Confusion matrix:\n107.0  13.0  \n18.0   62.0  \n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "import org.apache.spark.mllib.evaluation.MulticlassMetrics\npredictionAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[92] at rdd at &lt;console&gt;:40\nmetrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@1d6dd02b\n"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "\n",
    "// MODEL EVALUATION\n",
    "\n",
    "// For Metrics and Evaluation\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "\n",
    "// Need to convert to RDD to use this\n",
    "val predictionAndLabels = results.select($\"prediction\",$\"label\").as[(Double, Double)].rdd\n",
    "\n",
    "// Instantiate metrics object\n",
    "val metrics = new MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "// Confusion matrix\n",
    "println(\"Confusion matrix:\")\n",
    "println(metrics.confusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Precision(0.0) = 0.856\nPrecision(1.0) = 0.8266666666666667\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "labels: Array[Double] = Array(0.0, 1.0)\n"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "val labels = metrics.labels\n",
    "labels.foreach { l =>\n",
    "  println(s\"Precision($l) = \" + metrics.precision(l))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}