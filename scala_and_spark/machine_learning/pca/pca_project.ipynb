{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.22:4042\n",
       "SparkContext available as 'sc' (version = 3.0.0, master = local[*], app id = local-1601805491317)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Breast Cancer Wisconsin (Diagnostic) Database\n",
    "//\n",
    "// Notes\n",
    "// -----\n",
    "// Data Set Characteristics:\n",
    "//     :Number of Instances: 569\n",
    "//\n",
    "//     :Number of Attributes: 30 numeric, predictive attributes and the class\n",
    "//\n",
    "//     :Attribute Information:\n",
    "//         - radius (mean of distances from center to points on the perimeter)\n",
    "//         - texture (standard deviation of gray-scale values)\n",
    "//         - perimeter\n",
    "//         - area\n",
    "//         - smoothness (local variation in radius lengths)\n",
    "//         - compactness (perimeter^2 / area - 1.0)\n",
    "//         - concavity (severity of concave portions of the contour)\n",
    "//         - concave points (number of concave portions of the contour)\n",
    "//         - symmetry\n",
    "//         - fractal dimension (\"coastline approximation\" - 1)\n",
    "//\n",
    "//         The mean, standard error, and \"worst\" or largest (mean of the three\n",
    "//         largest values) of these features were computed for each image,\n",
    "//         resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
    "//         13 is Radius SE, field 23 is Worst Radius.\n",
    "//\n",
    "//         - class:\n",
    "//                 - WDBC-Malignant\n",
    "//                 - WDBC-Benign\n",
    "//\n",
    "//     :Summary Statistics:\n",
    "//\n",
    "//     ===================================== ======= ========\n",
    "//                                            Min     Max\n",
    "//     ===================================== ======= ========\n",
    "//     radius (mean):                         6.981   28.11\n",
    "//     texture (mean):                        9.71    39.28\n",
    "//     perimeter (mean):                      43.79   188.5\n",
    "//     area (mean):                           143.5   2501.0\n",
    "//     smoothness (mean):                     0.053   0.163\n",
    "//     compactness (mean):                    0.019   0.345\n",
    "//     concavity (mean):                      0.0     0.427\n",
    "//     concave points (mean):                 0.0     0.201\n",
    "//     symmetry (mean):                       0.106   0.304\n",
    "//     fractal dimension (mean):              0.05    0.097\n",
    "//     radius (standard error):               0.112   2.873\n",
    "//     texture (standard error):              0.36    4.885\n",
    "//     perimeter (standard error):            0.757   21.98\n",
    "//     area (standard error):                 6.802   542.2\n",
    "//     smoothness (standard error):           0.002   0.031\n",
    "//     compactness (standard error):          0.002   0.135\n",
    "//     concavity (standard error):            0.0     0.396\n",
    "//     concave points (standard error):       0.0     0.053\n",
    "//     symmetry (standard error):             0.008   0.079\n",
    "//     fractal dimension (standard error):    0.001   0.03\n",
    "//     radius (worst):                        7.93    36.04\n",
    "//     texture (worst):                       12.02   49.54\n",
    "//     perimeter (worst):                     50.41   251.2\n",
    "//     area (worst):                          185.2   4254.0\n",
    "//     smoothness (worst):                    0.071   0.223\n",
    "//     compactness (worst):                   0.027   1.058\n",
    "//     concavity (worst):                     0.0     1.252\n",
    "//     concave points (worst):                0.0     0.291\n",
    "//     symmetry (worst):                      0.156   0.664\n",
    "//     fractal dimension (worst):             0.055   0.208\n",
    "//     ===================================== ======= ========\n",
    "//\n",
    "//     :Missing Attribute Values: None\n",
    "//\n",
    "//     :Class Distribution: 212 - Malignant, 357 - Benign\n",
    "//\n",
    "//     :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
    "//\n",
    "//     :Donor: Nick Street\n",
    "//\n",
    "//     :Date: November, 1995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.ml.feature.{PCA, VectorAssembler, StandardScaler}\n",
       "import org.apache.spark.ml.linalg.Vectors\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Import Spark and Create a Spark Session\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "// Import PCA, VectorAssembler and StandardScaler from ml.feature\n",
    "import org.apache.spark.ml.feature.{PCA, VectorAssembler, StandardScaler}\n",
    "// import org.apache.spark.ml.feature.{VectorAssembler,StringIndexer,VectorIndexer,OneHotEncoder}\n",
    "\n",
    "// Import Vectors from ml.linalg\n",
    "import org.apache.spark.ml.linalg.Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5e1638d3\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a Spark session.\n",
    "val spark = SparkSession.builder().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mean radius: integer (nullable = true)\n",
      " |-- mean texture: double (nullable = true)\n",
      " |-- mean perimeter: double (nullable = true)\n",
      " |-- mean area: double (nullable = true)\n",
      " |-- mean smoothness: double (nullable = true)\n",
      " |-- mean compactness: double (nullable = true)\n",
      " |-- mean concavity: double (nullable = true)\n",
      " |-- mean concave points: double (nullable = true)\n",
      " |-- mean symmetry: double (nullable = true)\n",
      " |-- mean fractal dimension: double (nullable = true)\n",
      " |-- radius error: double (nullable = true)\n",
      " |-- texture error: double (nullable = true)\n",
      " |-- perimeter error: double (nullable = true)\n",
      " |-- area error: double (nullable = true)\n",
      " |-- smoothness error: double (nullable = true)\n",
      " |-- compactness error: double (nullable = true)\n",
      " |-- concavity error: double (nullable = true)\n",
      " |-- concave points error: double (nullable = true)\n",
      " |-- symmetry error: double (nullable = true)\n",
      " |-- fractal dimension error: double (nullable = true)\n",
      " |-- worst radius: double (nullable = true)\n",
      " |-- worst texture: double (nullable = true)\n",
      " |-- worst perimeter: double (nullable = true)\n",
      " |-- worst area: double (nullable = true)\n",
      " |-- worst smoothness: double (nullable = true)\n",
      " |-- worst compactness: double (nullable = true)\n",
      " |-- worst concavity: double (nullable = true)\n",
      " |-- worst concave points: double (nullable = true)\n",
      " |-- worst symmetry: double (nullable = true)\n",
      " |-- worst fractal dimension: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [mean radius: int, mean texture: double ... 28 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Use Spark to read in the cancer_data file.\n",
    "val data = spark.read\n",
    "    .option(\"header\",\"true\")\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "    .format(\"csv\")\n",
    "    .load(\"cancer_data\")\n",
    "\n",
    "// Print the schema of the data.\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "column_names: Array[String] = Array(mean radius, mean texture, mean perimeter, mean area, mean smoothness, mean compactness, mean concavity, mean concave points, mean symmetry, mean fractal dimension, radius error, texture error, perimeter error, area error, smoothness error, compactness error, concavity error, concave points error, symmetry error, fractal dimension error, worst radius, worst texture, worst perimeter, worst area, worst smoothness, worst compactness, worst concavity, worst concave points, worst symmetry, worst fractal dimension)\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val column_names = data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_6482d95f2f10, handleInvalid=error, numInputCols=30\n",
       "output: org.apache.spark.sql.DataFrame = [features: vector]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Use VectorAssembler to convert the input columns of the cancer data\n",
    "// to a single output column of an array called \"features\"\n",
    "// Set the input columns from which we are supposed to read the values.\n",
    "// Call this new object assembler.\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(column_names)\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "// Use the assembler to transform our DataFrame to a single column: features\n",
    "val output = assembler.transform(data).select($\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_2bacfdb23024\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Often its a good idea to normalize each feature to have unit standard\n",
    "// deviation and/or zero mean, when using PCA.\n",
    "// This is essentially a pre-step to PCA, but its not always necessary.\n",
    "\n",
    "// Look at the ml.feature documentation and figure out how to standardize\n",
    "// the cancer data set. Refer to the solutions for hints if you get stuck.\n",
    "\n",
    "// Use StandardScaler on the data\n",
    "// Create a new StandardScaler() object called scaler\n",
    "// Set the input to the features column and the ouput to a column called\n",
    "// scaledFeatures\n",
    "\n",
    "val scaler = new StandardScaler()\n",
    "    .setInputCol(\"features\")\n",
    "    .setOutputCol(\"scaledFeatures\")\n",
    "    .setWithStd(true)\n",
    "    .setWithMean(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scalerModel: org.apache.spark.ml.feature.StandardScalerModel = StandardScalerModel: uid=stdScal_2bacfdb23024, numFeatures=30, withMean=false, withStd=true\n",
       "scaledData: org.apache.spark.sql.DataFrame = [features: vector, scaledFeatures: vector]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Compute summary statistics by fitting the StandardScaler.\n",
    "// Basically create a new object called scalerModel by using scaler.fit()\n",
    "// on the output of the VectorAssembler\n",
    "val scalerModel = scaler.fit(output)\n",
    "\n",
    "// Normalize each feature to have unit standard deviation.\n",
    "// Use transform() off of this scalerModel object to create your scaledData\n",
    "val scaledData = scalerModel.transform(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pca: org.apache.spark.ml.feature.PCAModel = PCAModel: uid=pca_7669a40b137a, k=4\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Now its time to use PCA to reduce the features to some principal components\n",
    "\n",
    "// Create a new PCA() object that will take in the scaledFeatures\n",
    "// and output the pcs features, use 4 principal components\n",
    "// Then fit this to the scaledData\n",
    "val pca = (new PCA()\n",
    "    .setInputCol(\"scaledFeatures\")\n",
    "    .setOutputCol(\"pcaFeatures\")\n",
    "    .setK(4)\n",
    "    .fit(scaledData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         pcaFeatures|\n",
      "+--------------------+\n",
      "|[21.6219973823649...|\n",
      "|[15.1217370347583...|\n",
      "|[18.4325856097778...|\n",
      "|[18.9549565028940...|\n",
      "|[16.7333072691963...|\n",
      "|[14.6702143413857...|\n",
      "|[14.9731218113083...|\n",
      "|[14.5441067848488...|\n",
      "|[15.5980062310548...|\n",
      "|[17.9720563963992...|\n",
      "|[11.9024481321385...|\n",
      "|[15.1741616728687...|\n",
      "|[20.7357773767196...|\n",
      "|[13.1902413206624...|\n",
      "|[16.4793766691530...|\n",
      "|[16.3184525605226...|\n",
      "|[12.9382787109835...|\n",
      "|[16.8474451580445...|\n",
      "|[17.7858229434146...|\n",
      "|[11.5044884686410...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pcaDF: org.apache.spark.sql.DataFrame = [features: vector, scaledFeatures: vector ... 1 more field]\n",
       "result: org.apache.spark.sql.DataFrame = [pcaFeatures: vector]\n",
       "res2: Array[org.apache.spark.sql.Row] = Array([[21.62199738236499,8.516595739466716,-3.731847417579699,-0.41812449701477833]])\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Once your pca has been created and fit, transform the scaledData\n",
    "// Call this new dataframe pcaDF\n",
    "val pcaDF = pca.transform(scaledData)\n",
    "\n",
    "// Show the new pcaFeatures\n",
    "val result = pcaDF.select(\"pcaFeatures\")\n",
    "result.show()\n",
    "\n",
    "// Use .head() to confirm that your output column Array of pcaFeatures\n",
    "// only has 4 principal components\n",
    "result.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "miscellanea_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
