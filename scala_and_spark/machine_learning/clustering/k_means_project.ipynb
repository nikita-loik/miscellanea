{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.22:4041\n",
       "SparkContext available as 'sc' (version = 3.0.0, master = local[*], app id = local-1601800385234)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Try to cluster clients of a Wholesale Distributor\n",
    "// based off of the sales of some product categories\n",
    "\n",
    "// Source of the Data\n",
    "//http://archive.ics.uci.edu/ml/datasets/Wholesale+customers\n",
    "\n",
    "// Here is the info on the data:\n",
    "// 1)\tFRESH: annual spending (m.u.) on fresh products (Continuous);\n",
    "// 2)\tMILK: annual spending (m.u.) on milk products (Continuous);\n",
    "// 3)\tGROCERY: annual spending (m.u.)on grocery products (Continuous);\n",
    "// 4)\tFROZEN: annual spending (m.u.)on frozen products (Continuous)\n",
    "// 5)\tDETERGENTS_PAPER: annual spending (m.u.) on detergents and paper products (Continuous)\n",
    "// 6)\tDELICATESSEN: annual spending (m.u.)on and delicatessen products (Continuous);\n",
    "// 7)\tCHANNEL: customers Channel - Horeca (Hotel/Restaurant/Cafe) or Retail channel (Nominal)\n",
    "// 8)\tREGION: customers Region- Lisnon, Oporto or Other (Nominal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, VectorIndexer, OneHotEncoder}\n",
       "import org.apache.spark.ml.linalg.Vectors\n",
       "import org.apache.log4j._\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@116f08e5\n",
       "import org.apache.spark.ml.clustering.KMeans\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Import SparkSession\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "// Import VectorAssembler and Vectors\n",
    "import org.apache.spark.ml.feature.{VectorAssembler,StringIndexer,VectorIndexer,OneHotEncoder}\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "\n",
    "// Set the Error reporting\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "\n",
    "\n",
    "// Create a Spark Session Instance\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "\n",
    "// Import Kmeans clustering Algorithm\n",
    "import org.apache.spark.ml.clustering.KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [Channel: int, Region: int ... 6 more fields]\n",
       "feature_data: org.apache.spark.sql.DataFrame = [Fresh: int, Milk: int ... 4 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load the Wholesale Customers Data\n",
    "val data = spark.read\n",
    "    .option(\"header\",\"true\")\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "    .csv(\"wholesale_customers_data.csv\")\n",
    "// Select the following columns for the training set:\n",
    "// Fresh, Milk, Grocery, Frozen, Detergents_Paper, Delicassen\n",
    "// Cal this new subset feature_data\n",
    "val feature_data = data\n",
    "    .select($\"Fresh\", $\"Milk\", $\"Grocery\", $\"Frozen\", $\"Detergents_Paper\", $\"Delicassen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_ee918c704c7e, handleInvalid=error, numInputCols=6\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a new VectorAssembler object called assembler for the feature\n",
    "// columns as the input Set the output column to be called features\n",
    "// Remember there is no Label column\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "    .setInputCols(\n",
    "        Array(\"Fresh\", \"Milk\", \"Grocery\", \"Frozen\", \"Detergents_Paper\", \"Delicassen\"))\n",
    "    .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training_data: org.apache.spark.sql.DataFrame = [features: vector]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Use the assembler object to transform the feature_data\n",
    "// Call this new data training_data\n",
    "val training_data = assembler\n",
    "    .transform(feature_data)\n",
    "    .select(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kmeans: org.apache.spark.ml.clustering.KMeans = kmeans_0cd8d351105e\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a Kmeans Model with K=3\n",
    "val kmeans = new KMeans().setK(8).setSeed(1L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.clustering.KMeansModel = KMeansModel: uid=kmeans_0cd8d351105e, k=8, distanceMeasure=euclidean, numFeatures=6\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Fit that model to the training_data\n",
    "val model = kmeans.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [features: vector, prediction: int]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Make predictions\n",
    "val predictions = model.transform(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.5260446829409514\n",
      "Cluster Centers: \n",
      "[6539.087962962963,3006.1157407407404,3540.4675925925926,2709.949074074074,949.2037037037037,973.8194444444443]\n",
      "[23710.849056603773,3882.056603773585,5169.311320754717,3772.4433962264147,1118.264150943396,1693.1037735849056]\n",
      "[22925.0,73498.0,32114.0,987.0,20070.0,903.0]\n",
      "[40204.0,46314.0,57584.5,5518.0,25436.0,4241.0]\n",
      "[16117.0,46197.0,92780.0,1026.0,40827.0,2944.0]\n",
      "[4083.797619047619,9649.380952380952,15284.261904761905,1383.642857142857,6546.559523809524,1461.404761904762]\n",
      "[56453.307692307695,10026.23076923077,8739.0,15518.076923076924,1258.2307692307693,7170.538461538462]\n",
      "[9291.529411764706,19783.882352941175,32254.470588235294,2083.0588235294117,15662.117647058823,3107.3529411764707]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.ClusteringEvaluator\n",
       "evaluator: org.apache.spark.ml.evaluation.ClusteringEvaluator = ClusteringEvaluator: uid=cluEval_7697b05b2dfa, metricName=silhouette, distanceMeasure=squaredEuclidean\n",
       "silhouette: Double = 0.5260446829409514\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Evaluate clustering by computing Silhouette score\n",
    "import org.apache.spark.ml.evaluation.ClusteringEvaluator\n",
    "val evaluator = new ClusteringEvaluator()\n",
    "\n",
    "val silhouette = evaluator.evaluate(predictions)\n",
    "println(s\"Silhouette with squared euclidean distance = $silhouette\")\n",
    "\n",
    "// Shows the result.\n",
    "println(\"Cluster Centers: \")\n",
    "model.clusterCenters.foreach(println)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "miscellanea_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
