{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "import org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\nimport org.apache.log4j._\nimport org.apache.spark.sql.SparkSession\n"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
    "\n",
    "// Set error reportings: see less warnings\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "\n",
    "// Start a simple Spark Session\n",
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@14dedc39\ndata: org.apache.spark.sql.DataFrame = [Avg Area Income: double, Avg Area House Age: double ... 4 more fields]\n"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "val spark = SparkSession.builder().getOrCreate()\n",
    "\n",
    "// Prepare training and test data.\n",
    "val data = spark.read\n",
    "  .option(\"header\",\"true\")\n",
    "  .option(\"inferSchema\",\"true\")\n",
    "  .format(\"csv\")\n",
    "  .load(\"usa_housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- Avg Area Income: double (nullable = true)\n |-- Avg Area House Age: double (nullable = true)\n |-- Avg Area Number of Rooms: double (nullable = true)\n |-- Avg Area Number of Bedrooms: double (nullable = true)\n |-- Area Population: double (nullable = true)\n |-- Price: double (nullable = true)\n\n"
    }
   ],
   "source": [
    "// Check out the Data\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\nExample Data Row\nAvg Area House Age\n5.682861321615587\n\n\nAvg Area Number of Rooms\n7.009188142792237\n\n\nAvg Area Number of Bedrooms\n4.09\n\n\nArea Population\n23086.800502686456\n\n\nPrice\n1059033.5578701235\n\n\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "colnames: Array[String] = Array(Avg Area Income, Avg Area House Age, Avg Area Number of Rooms, Avg Area Number of Bedrooms, Area Population, Price)\nfirstrow: org.apache.spark.sql.Row = [79545.45857431678,5.682861321615587,7.009188142792237,4.09,23086.800502686456,1059033.5578701235]\n"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "// See an example of what the data looks like\n",
    "// by printing out a Row\n",
    "val colnames = data.columns\n",
    "val firstrow = data.head(1)(0)\n",
    "println(\"\\n\")\n",
    "println(\"Example Data Row\")\n",
    "for(ind <- Range(1,colnames.length)){\n",
    "  println(colnames(ind))\n",
    "  println(firstrow(ind))\n",
    "  println(\"\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "import org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.linalg.Vectors\n"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "// This will allow us to join multiple feature columns\n",
    "// into a single column of an array of feautre values\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.linalg.Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "df: org.apache.spark.sql.DataFrame = [label: double, Avg Area Income: double ... 3 more fields]\n"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "// Rename Price to label column for naming convention.\n",
    "// Grab only numerical columns from the data\n",
    "val df = data.select(\n",
    "    data(\"Price\").as(\"label\"),\n",
    "    $\"Avg Area Income\",\n",
    "    $\"Avg Area House Age\",\n",
    "    $\"Avg Area Number of Rooms\",\n",
    "    $\"Area Population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_291872167add, handleInvalid=error, numInputCols=4\n"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "// An assembler converts the input values to a vector\n",
    "// A vector is what the ML algorithm reads to train a model\n",
    "\n",
    "// Set the input columns from which we are supposed to read the values\n",
    "// Set the name of the column where the vector will be stored\n",
    "val assembler = new VectorAssembler()\n",
    "    .setInputCols(Array(\n",
    "        \"Avg Area Income\",\n",
    "        \"Avg Area House Age\",\n",
    "        \"Avg Area Number of Rooms\",\n",
    "        \"Area Population\"))\n",
    "    .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "output: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "// Use the assembler to transform our DataFrame to the two columns\n",
    "val output = assembler.transform(df).select($\"label\",$\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------------------+--------------------+\n|             label|            features|\n+------------------+--------------------+\n|1059033.5578701235|[79545.4585743167...|\n|  1505890.91484695|[79248.6424548256...|\n|1058987.9878760849|[61287.0671786567...|\n|1260616.8066294468|[63345.2400462279...|\n| 630943.4893385402|[59982.1972257080...|\n|1068138.0743935304|[80175.7541594853...|\n|1502055.8173744078|[64698.4634278877...|\n|1573936.5644777215|[78394.3392775308...|\n| 798869.5328331633|[59927.6608133496...|\n|1545154.8126419624|[81885.9271840956...|\n| 1707045.722158058|[80527.4720829228...|\n| 663732.3968963273|[50593.6954970428...|\n|1042814.0978200928|[39033.8092369823...|\n|1291331.5184858206|[73163.6634410467...|\n|1402818.2101658515|[69391.3801843616...|\n|1306674.6599511993|[73091.8667458232...|\n|1556786.6001947748|[79706.9630576574...|\n| 528485.2467305964|[61929.0770180892...|\n|1019425.9367578316|[63508.1942994299...|\n|1030591.4292116085|[62085.2764034048...|\n+------------------+--------------------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "lr: org.apache.spark.ml.regression.LinearRegression = linReg_b65803f26fc7\n"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "// Create a Linear Regression Model object\n",
    "val lr = new LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "lrModel: org.apache.spark.ml.regression.LinearRegressionModel = LinearRegressionModel: uid=linReg_b65803f26fc7, numFeatures=4\n"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "// Fit the model to the data\n",
    "\n",
    "// Note: Later we will see why we should split\n",
    "// the data first, but for now we will fit to all the data.\n",
    "val lrModel = lr.fit(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Coefficients: [21.58274357311781,165657.8724329605,121598.16461647583,15.196119819750825] Intercept: -2637560.67254761\n"
    }
   ],
   "source": [
    "// Print the coefficients and intercept for linear regression\n",
    "println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "numIterations: 1\nobjectiveHistory: List(0.0)\n+-------------------+\n|          residuals|\n+-------------------+\n|-164759.92057681922|\n|  9690.547545112902|\n| -193522.9705765734|\n|  139506.9957614462|\n| -214819.1296369878|\n|  147.1563499146141|\n|-170004.15564831533|\n| 1675.3336070652585|\n|  30782.38727523212|\n|  79276.89880586648|\n| -64967.26304096077|\n| 34514.368293934385|\n|   89443.8314024884|\n|  -16448.1027878148|\n|  95327.46257374831|\n|  65660.76816228265|\n| 31450.784453772707|\n| 42636.212100361125|\n|-115709.42082964187|\n|-167589.65318883688|\n+-------------------+\nonly showing top 20 rows\n\nRMSE: 101108.8122249438\nMSE: 1.0222991909538944E10\nr2: 0.9179976891524392\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "trainingSummary: org.apache.spark.ml.regression.LinearRegressionTrainingSummary = org.apache.spark.ml.regression.LinearRegressionTrainingSummary@62d03e54\n"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "// Summarize the model over the training set and print out some metrics!\n",
    "// Explore this in the spark-shell for more methods to call\n",
    "val trainingSummary = lrModel.summary\n",
    "\n",
    "println(s\"numIterations: ${trainingSummary.totalIterations}\")\n",
    "println(s\"objectiveHistory: ${trainingSummary.objectiveHistory.toList}\")\n",
    "\n",
    "trainingSummary.residuals.show()\n",
    "\n",
    "println(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")\n",
    "println(s\"MSE: ${trainingSummary.meanSquaredError}\")\n",
    "println(s\"r2: ${trainingSummary.r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}