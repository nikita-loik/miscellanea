{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.22:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1586260261553)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "import org.apache.spark.ml.regression.LinearRegression\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
       "import org.apache.log4j._\n",
       "import org.apache.spark.sql.SparkSession\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
    "\n",
    "// See less warnings\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "\n",
    "// Start a simple Spark Session\n",
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@60986f43\n",
       "data: org.apache.spark.sql.DataFrame = [Avg Area Income: double, Avg Area House Age: double ... 4 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder().getOrCreate()\n",
    "\n",
    "// Prepare training and test data.\n",
    "val data = spark.read\n",
    "  .option(\"header\",\"true\")\n",
    "  .option(\"inferSchema\",\"true\")\n",
    "  .format(\"csv\")\n",
    "  .load(\"usa_housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Avg Area Income: double (nullable = true)\n",
      " |-- Avg Area House Age: double (nullable = true)\n",
      " |-- Avg Area Number of Rooms: double (nullable = true)\n",
      " |-- Avg Area Number of Bedrooms: double (nullable = true)\n",
      " |-- Area Population: double (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Check out the Data\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Example Data Row\n",
      "Avg Area House Age\n",
      "5.682861321615587\n",
      "\n",
      "\n",
      "Avg Area Number of Rooms\n",
      "7.009188142792237\n",
      "\n",
      "\n",
      "Avg Area Number of Bedrooms\n",
      "4.09\n",
      "\n",
      "\n",
      "Area Population\n",
      "23086.800502686456\n",
      "\n",
      "\n",
      "Price\n",
      "1059033.5578701235\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "colnames: Array[String] = Array(Avg Area Income, Avg Area House Age, Avg Area Number of Rooms, Avg Area Number of Bedrooms, Area Population, Price)\n",
       "firstrow: org.apache.spark.sql.Row = [79545.45857431678,5.682861321615587,7.009188142792237,4.09,23086.800502686456,1059033.5578701235]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// See an example of what the data looks like\n",
    "// by printing out a Row\n",
    "val colnames = data.columns\n",
    "val firstrow = data.head(1)(0)\n",
    "println(\"\\n\")\n",
    "println(\"Example Data Row\")\n",
    "for(ind <- Range(1,colnames.length)){\n",
    "  println(colnames(ind))\n",
    "  println(firstrow(ind))\n",
    "  println(\"\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.linalg.Vectors\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// This will allow us to join multiple feature columns\n",
    "// into a single column of an array of feautre values\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.linalg.Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [label: double, Avg Area Income: double ... 3 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Rename Price to label column for naming convention.\n",
    "// Grab only numerical columns from the data\n",
    "val df = data.select(\n",
    "    data(\"Price\").as(\"label\"),\n",
    "    $\"Avg Area Income\",\n",
    "    $\"Avg Area House Age\",\n",
    "    $\"Avg Area Number of Rooms\",\n",
    "    $\"Area Population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_acc092b68fdc\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// An assembler converts the input values to a vector\n",
    "// A vector is what the ML algorithm reads to train a model\n",
    "\n",
    "// Set the input columns from which we are supposed to read the values\n",
    "// Set the name of the column where the vector will be stored\n",
    "val assembler = new VectorAssembler()\n",
    "    .setInputCols(Array(\n",
    "        \"Avg Area Income\",\n",
    "        \"Avg Area House Age\",\n",
    "        \"Avg Area Number of Rooms\",\n",
    "        \"Area Population\"))\n",
    "    .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Use the assembler to transform our DataFrame to the two columns\n",
    "val output = assembler.transform(df).select($\"label\",$\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|             label|            features|\n",
      "+------------------+--------------------+\n",
      "|1059033.5578701235|[79545.4585743167...|\n",
      "|  1505890.91484695|[79248.6424548256...|\n",
      "|1058987.9878760849|[61287.0671786567...|\n",
      "|1260616.8066294468|[63345.2400462279...|\n",
      "| 630943.4893385402|[59982.1972257080...|\n",
      "|1068138.0743935304|[80175.7541594853...|\n",
      "|1502055.8173744078|[64698.4634278877...|\n",
      "|1573936.5644777215|[78394.3392775308...|\n",
      "| 798869.5328331633|[59927.6608133496...|\n",
      "|1545154.8126419624|[81885.9271840956...|\n",
      "| 1707045.722158058|[80527.4720829228...|\n",
      "| 663732.3968963273|[50593.6954970428...|\n",
      "|1042814.0978200928|[39033.8092369823...|\n",
      "|1291331.5184858206|[73163.6634410467...|\n",
      "|1402818.2101658515|[69391.3801843616...|\n",
      "|1306674.6599511993|[73091.8667458232...|\n",
      "|1556786.6001947748|[79706.9630576574...|\n",
      "| 528485.2467305964|[61929.0770180892...|\n",
      "|1019425.9367578316|[63508.1942994299...|\n",
      "|1030591.4292116085|[62085.2764034048...|\n",
      "+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.regression.LinearRegression = linReg_f989e16d785f\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a Linear Regression Model object\n",
    "val lr = new LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrModel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_f989e16d785f\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Fit the model to the data\n",
    "\n",
    "// Note: Later we will see why we should split\n",
    "// the data first, but for now we will fit to all the data.\n",
    "val lrModel = lr.fit(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [21.58274357311781,165657.8724329605,121598.16461647583,15.196119819750825] Intercept: -2637560.67254761\n"
     ]
    }
   ],
   "source": [
    "// Print the coefficients and intercept for linear regression\n",
    "println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numIterations: 1\n",
      "objectiveHistory: List(0.0)\n",
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "|-164759.92057681922|\n",
      "|  9690.547545112902|\n",
      "| -193522.9705765734|\n",
      "|  139506.9957614462|\n",
      "| -214819.1296369878|\n",
      "|  147.1563499146141|\n",
      "|-170004.15564831533|\n",
      "| 1675.3336070652585|\n",
      "|  30782.38727523212|\n",
      "|  79276.89880586648|\n",
      "| -64967.26304096077|\n",
      "| 34514.368293934385|\n",
      "|   89443.8314024884|\n",
      "|  -16448.1027878148|\n",
      "|  95327.46257374831|\n",
      "|  65660.76816228265|\n",
      "| 31450.784453772707|\n",
      "| 42636.212100361125|\n",
      "|-115709.42082964187|\n",
      "|-167589.65318883688|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "RMSE: 101108.8122249438\n",
      "MSE: 1.0222991909538944E10\n",
      "r2: 0.9179976891524392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trainingSummary: org.apache.spark.ml.regression.LinearRegressionTrainingSummary = org.apache.spark.ml.regression.LinearRegressionTrainingSummary@69fcaef6\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Summarize the model over the training set and print out some metrics!\n",
    "// Explore this in the spark-shell for more methods to call\n",
    "val trainingSummary = lrModel.summary\n",
    "\n",
    "println(s\"numIterations: ${trainingSummary.totalIterations}\")\n",
    "println(s\"objectiveHistory: ${trainingSummary.objectiveHistory.toList}\")\n",
    "\n",
    "trainingSummary.residuals.show()\n",
    "\n",
    "println(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")\n",
    "println(s\"MSE: ${trainingSummary.meanSquaredError}\")\n",
    "println(s\"r2: ${trainingSummary.r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "miscellanea_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
