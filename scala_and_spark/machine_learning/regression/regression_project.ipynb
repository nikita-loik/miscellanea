{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "import org.apache.spark.ml.regression.LinearRegression\nimport org.apache.log4j._\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.linalg.Vectors\n"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "// Import LinearRegression\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "\n",
    "// Set the Error reporting\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "\n",
    "// Start a simple Spark Session\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "// Import VectorAssembler and Vectors\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.linalg.Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- Email: string (nullable = true)\n |-- Address: string (nullable = true)\n |-- Avatar: string (nullable = true)\n |-- Avg Session Length: double (nullable = true)\n |-- Time on App: double (nullable = true)\n |-- Time on Website: double (nullable = true)\n |-- Length of Membership: double (nullable = true)\n |-- Yearly Amount Spent: double (nullable = true)\n\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6ef07d6\ndata: org.apache.spark.sql.DataFrame = [Email: string, Address: string ... 6 more fields]\n"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "val spark = SparkSession.builder().getOrCreate()\n",
    "\n",
    "// Use Spark to read in the ecommerce_customers csv file.\n",
    "val data = spark.read\n",
    "    .option(\"header\",\"true\")\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "    .format(\"csv\")\n",
    "    .load(\"ecommerce_customers\")\n",
    "// Print the Schema of the DataFrame\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\nexample data row\nAddress\n MI 82180-9605&quot;\n\n\nAvatar\nViolet\n\n\nAvg Session Length\n34.49726772511229\n\n\nTime on App\n12.655651149166752\n\n\nTime on Website\n39.57766801952616\n\n\nLength of Membership\n4.082620632952961\n\n\nYearly Amount Spent\n587.9510539684005\n\n\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "col_names: Array[String] = Array(Email, Address, Avatar, Avg Session Length, Time on App, Time on Website, Length of Membership, Yearly Amount Spent)\nfirst_row: org.apache.spark.sql.Row = [Wrightmouth, MI 82180-9605&quot;,Violet,34.49726772511229,12.655651149166752,39.57766801952616,4.082620632952961,587.9510539684005]\n"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "val col_names = data.columns\n",
    "val first_row = data.head(2)(1)\n",
    "println(\"\\n\")\n",
    "println(\"example data row\")\n",
    "for (ind <- Range(1, col_names.length)){\n",
    "println(col_names(ind))\n",
    "println(first_row(ind))\n",
    "println(\"\\n\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "res2: Array[org.apache.spark.sql.Row] = Array([mstephenson@fernandez.com,835 Frank Tunnel,null,null,null,null,null,null], [Wrightmouth, MI 82180-9605&quot;,Violet,34.49726772511229,12.655651149166752,39.57766801952616,4.082620632952961,587.9510539684005])\n"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "res3: org.apache.spark.sql.Row = [mstephenson@fernandez.com,835 Frank Tunnel,null,null,null,null,null,null]\n"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "// In PySpark, if your dataset is small (can fit into memory of driver), you can do\n",
    "data.take(2)(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "res4: Array[String] = Array(Email, Address, Avatar, Avg Session Length, Time on App, Time on Website, Length of Membership, Yearly Amount Spent)\n"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "df: org.apache.spark.sql.DataFrame = [label: double, Avg Session Length: double ... 3 more fields]\ndf_no_null: org.apache.spark.sql.DataFrame = [label: double, Avg Session Length: double ... 3 more fields]\n"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "// Rename the Yearly Amount Spent Column as \"label\"\n",
    "// Also grab only the numerical columns from the data\n",
    "// Set all of this as a new dataframe called df\n",
    "val df = data.select(\n",
    "    data(\"Yearly Amount Spent\").as(\"label\"),\n",
    "//     $\"Email\",\n",
    "//     $\"Address\",\n",
    "//     $\"Avatar\",\n",
    "    $\"Avg Session Length\",\n",
    "    $\"Time on App\",\n",
    "    $\"Time on Website\",\n",
    "    $\"Length of Membership\")\n",
    "\n",
    "val df_no_null = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- label: double (nullable = true)\n |-- Avg Session Length: double (nullable = true)\n |-- Time on App: double (nullable = true)\n |-- Time on Website: double (nullable = true)\n |-- Length of Membership: double (nullable = true)\n\n"
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_b8342e89bdea, handleInvalid=error, numInputCols=4\n"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "// Set the input columns from which we are supposed to read the values.\n",
    "// Call this new object assembler\n",
    "// Use VectorAssembler to convert the input columns of df\n",
    "// to a single output column of an array called \"features\"\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "    .setInputCols(Array(\n",
    "        \"Avg Session Length\",\n",
    "        \"Time on App\",\n",
    "        \"Time on Website\",\n",
    "        \"Length of Membership\"))\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "// An assembler converts the input values to a vector\n",
    "// A vector is what the ML algorithm reads to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "output: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "// Use the assembler to transform our DataFrame to the two columns: label and features\n",
    "val output = assembler\n",
    "    .transform(df_no_null)\n",
    "    .select(\n",
    "        $\"label\",\n",
    "        $\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------------------+--------------------+\n|             label|            features|\n+------------------+--------------------+\n| 587.9510539684005|[34.4972677251122...|\n| 392.2049334443264|[31.9262720263601...|\n|487.54750486747207|[33.0009147556426...|\n| 581.8523440352178|[34.3055566297555...|\n| 599.4060920457634|[33.3306725236463...|\n|  637.102447915074|[33.8710378793419...|\n| 521.5721747578274|[32.0215955013870...|\n| 570.2004089636195|[33.9877728956856...|\n| 492.6060127179966|[33.9925727749537...|\n|408.64035107262754|[29.5324289670579...|\n| 573.4158673313865|[33.1903340437226...|\n|470.45273330095546|[32.3879758531538...|\n| 461.7807421962299|[30.7377203726281...|\n|457.84769594494855|[32.1253868972878...|\n| 407.7045475495441|[32.3388993230671...|\n|452.31567548003545|[32.1878120459321...|\n|  605.061038804892|[32.6178560628234...|\n| 534.7057438060227|[32.9127851111597...|\n| 700.9170916173961|[34.5075509985266...|\n|423.17999168059777|[33.0293319535068...|\n+------------------+--------------------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "lr: org.apache.spark.ml.regression.LinearRegression = linReg_6f4af6b46b81\n"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "// Create a Linear Regression Model object\n",
    "val lr = new LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "lrModel: org.apache.spark.ml.regression.LinearRegressionModel = LinearRegressionModel: uid=linReg_6f4af6b46b81, numFeatures=4\n"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "// Fit the model to the data and call this model lrModel\n",
    "val lrModel = lr.fit(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Coefficients: [25.856618008426018,38.765463575921366,0.6400889131953854,61.646207880881704], Intercept: -1063.872486876065\n"
    }
   ],
   "source": [
    "// Print the coefficients and intercept for linear regression\n",
    "println(s\"Coefficients: ${lrModel.coefficients}, Intercept: ${lrModel.intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-------------------+\n|          residuals|\n+-------------------+\n| -7.772623457978966|\n|  11.82555250437241|\n|-17.879141870095964|\n| 11.081305443736483|\n|   7.32385327834254|\n|-1.7670434550307164|\n|  4.558311825657142|\n| 10.520236692071762|\n|-16.417217125194043|\n| 10.585380731138741|\n| 12.029113007876617|\n|  9.946068850417475|\n| 10.837290579785645|\n| 20.531726174145206|\n| -4.098013724803593|\n| -4.499385360994438|\n|  8.863806504862396|\n|-0.6646742992182908|\n|-3.0793993430667115|\n|-15.022236568735764|\n+-------------------+\nonly showing top 20 rows\n\nRMSE: 9.925307424896866\nMSE: 98.51172747871286\nr2: 0.9839070379164451\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "trainingSummary: org.apache.spark.ml.regression.LinearRegressionTrainingSummary = org.apache.spark.ml.regression.LinearRegressionTrainingSummary@7b4bf261\n"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "// Summarize the model over the training set and print out some metrics!\n",
    "// Use the .summary method off your model to create an object\n",
    "// called trainingSummary\n",
    "val trainingSummary = lrModel.summary\n",
    "// Show the residuals, the RMSE, the MSE, and the R^2 Values.\n",
    "trainingSummary.residuals.show()\n",
    "println(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")\n",
    "println(s\"MSE: ${trainingSummary.meanSquaredError}\")\n",
    "println(s\"r2: ${trainingSummary.r2}\")\n",
    "\n",
    "\n",
    "// Great Job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "spylon-kernel",
   "display_name": "spylon-kernel",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}