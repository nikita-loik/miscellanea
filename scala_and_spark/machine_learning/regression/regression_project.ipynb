{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.22:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1591260775836)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.LinearRegression\n",
       "import org.apache.log4j._\n",
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.linalg.Vectors\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Import LinearRegression\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "// Optional: Use the following code below to set the Error reporting\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "// Start a simple Spark Session\n",
    "import org.apache.spark.sql.SparkSession\n",
    "// Import VectorAssembler and Vectors\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.linalg.Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Avatar: string (nullable = true)\n",
      " |-- Avg Session Length: double (nullable = true)\n",
      " |-- Time on App: double (nullable = true)\n",
      " |-- Time on Website: double (nullable = true)\n",
      " |-- Length of Membership: double (nullable = true)\n",
      " |-- Yearly Amount Spent: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@39896f9c\n",
       "data: org.apache.spark.sql.DataFrame = [Email: string, Address: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder().getOrCreate()\n",
    "\n",
    "// Use Spark to read in the ecommerce_customers csv file.\n",
    "val data = spark.read\n",
    "    .option(\"header\",\"true\")\n",
    "    .option(\"inferSchema\",\"true\")\n",
    "    .format(\"csv\")\n",
    "    .load(\"ecommerce_customers\")\n",
    "// Print the Schema of the DataFrame\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[org.apache.spark.sql.Row] = Array([mstephenson@fernandez.com,835 Frank Tunnel,null,null,null,null,null,null])\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// In PySpark, if your dataset is small (can fit into memory of driver), you can do\n",
    "data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[String] = Array(Email, Address, Avatar, Avg Session Length, Time on App, Time on Website, Length of Membership, Yearly Amount Spent)\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [label: double, Avg Session Length: double ... 3 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Rename the Yearly Amount Spent Column as \"label\"\n",
    "// Also grab only the numerical columns from the data\n",
    "// Set all of this as a new dataframe called df\n",
    "val df = data.select(\n",
    "    data(\"Yearly Amount Spent\").as(\"label\"),\n",
    "//     $\"Email\",\n",
    "//     $\"Address\",\n",
    "//     $\"Avatar\",\n",
    "    $\"Avg Session Length\",\n",
    "    $\"Time on App\",\n",
    "    $\"Time on Website\",\n",
    "    $\"Length of Membership\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- Avg Session Length: double (nullable = true)\n",
      " |-- Time on App: double (nullable = true)\n",
      " |-- Time on Website: double (nullable = true)\n",
      " |-- Length of Membership: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_2b96c86af58e\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Set the input columns from which we are supposed to read the values.\n",
    "// Call this new object assembler\n",
    "// Use VectorAssembler to convert the input columns of df\n",
    "// to a single output column of an array called \"features\"\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "    .setInputCols(Array(\n",
    "        \"Avg Session Length\",\n",
    "        \"Time on App\",\n",
    "        \"Time on Website\",\n",
    "        \"Length of Membership\"))\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "// An assembler converts the input values to a vector\n",
    "// A vector is what the ML algorithm reads to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Use the assembler to transform our DataFrame to the two columns: label and features\n",
    "val output = assembler.transform(df).select($\"label\",$\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 4, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function(: (struct<Avg Session Length:double,Time on App:double,Time on Website:double,Length of Membership:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 4, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function(: (struct<Avg Session Length:double,Time on App:double,Time on Website:double,Length of Membership:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)",
      "\tat java.base/java.lang.Thread.run(Thread.java:830)",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)",
      "\t... 21 more",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)",
      "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)",
      "  at scala.Option.foreach(Option.scala:257)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)",
      "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)",
      "  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2550)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2764)",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:751)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:710)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:719)",
      "  ... 39 elided",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function(: (struct<Avg Session Length:double,Time on App:double,Time on Website:double,Length of Membership:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:123)",
      "  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)",
      "  ... 1 more",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".",
      "  at org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)",
      "  at org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)",
      "  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)",
      "  at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)",
      "  at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)",
      "  at org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)",
      "  at org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)",
      "  ... 21 more",
      ""
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.regression.LinearRegression = linReg_fde2ed459267\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a Linear Regression Model object\n",
    "val lr = new LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 5, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function(: (struct<Avg Session Length:double,Time on App:double,Time on Website:double,Length of Membership:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 5, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function(: (struct<Avg Session Length:double,Time on App:double,Time on Website:double,Length of Membership:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)",
      "\tat java.base/java.lang.Thread.run(Thread.java:830)",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)",
      "\t... 21 more",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)",
      "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)",
      "  at scala.Option.foreach(Option.scala:257)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)",
      "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)",
      "  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2550)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2557)",
      "  at org.apache.spark.sql.Dataset.first(Dataset.scala:2564)",
      "  at org.apache.spark.ml.regression.LinearRegression$$anonfun$train$1.apply(LinearRegression.scala:322)",
      "  at org.apache.spark.ml.regression.LinearRegression$$anonfun$train$1.apply(LinearRegression.scala:320)",
      "  at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)",
      "  at scala.util.Try$.apply(Try.scala:192)",
      "  at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)",
      "  at org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:320)",
      "  at org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:177)",
      "  at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)",
      "  ... 39 elided",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function(: (struct<Avg Session Length:double,Time on App:double,Time on Website:double,Length of Membership:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:123)",
      "  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)",
      "  ... 1 more",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"keep\". Consider",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".",
      "  at org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:287)",
      "  at org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:255)",
      "  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)",
      "  at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)",
      "  at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:255)",
      "  at org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:144)",
      "  at org.apache.spark.ml.feature.VectorAssembler$$anonfun$4.apply(VectorAssembler.scala:143)",
      "  ... 21 more",
      ""
     ]
    }
   ],
   "source": [
    "// Fit the model to the data and call this model lrModel\n",
    "val lrModel = lr.fit(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "34: error: not found: value lrModel",
     "output_type": "error",
     "traceback": [
      "<console>:34: error: not found: value lrModel",
      "       println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")",
      "                                 ^",
      "<console>:34: error: not found: value lrModel",
      "       println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")",
      "                                                                    ^",
      ""
     ]
    }
   ],
   "source": [
    "// Print the coefficients and intercept for linear regression\n",
    "println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Summarize the model over the training set and print out some metrics!\n",
    "// Use the .summary method off your model to create an object\n",
    "// called trainingSummary\n",
    "\n",
    "// Show the residuals, the RMSE, the MSE, and the R^2 Values.\n",
    "\n",
    "\n",
    "// Great Job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "miscellanea_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
