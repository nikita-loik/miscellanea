{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os, sys, inspect\n",
    "import csv\n",
    "import json\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import collections\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "working_dir = os.path.dirname(\n",
    "    os.path.abspath(\n",
    "        inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(working_dir)\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "from scripts_python import scraper as scr\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Chrome - Help - About Google Chrome\n",
    "2. check version; e.g. `Version 79.0.3945.117 (Official Build) (64-bit)`\n",
    "3. [download relevant ChromeDriver](https://chromedriver.chromium.org/downloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get credentials.\n",
    "with open('../ignorabilia/glassdoor_scraper.csv', mode='r') as f_in:\n",
    "    credentials = {k: v for k, v in csv.reader(f_in)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.glassdoor.co.uk\n"
     ]
    }
   ],
   "source": [
    "# Open browser.\n",
    "browser = scr.get_web_browser()\n",
    "browser.implicitly_wait(10)\n",
    "scr.load_main_page(browser=browser)\n",
    "\n",
    "# Accept cookies.\n",
    "scr.accept_cookies(browser=browser)\n",
    "\n",
    "# Signt in to glassdoor.\n",
    "scr.sign_in(\n",
    "    browser=browser,\n",
    "    e_mail=credentials['e_mail'],\n",
    "    password=credentials['password'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the search for particular keywords and location.\n",
    "scr.do_search(\n",
    "        browser=browser,\n",
    "        keywords='data scientist',\n",
    "        location='Paris (France)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust filters, including jobtype, post_age, and radius (from location centre).\n",
    "scr.set_filters(\n",
    "        browser=browser,\n",
    "        jobtype='fulltime',\n",
    "        post_age='7',\n",
    "        radius='5',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tjobs 437\n",
      "\tpages 12\n"
     ]
    }
   ],
   "source": [
    "scraped_results = []\n",
    "search_date = datetime.datetime.now()\n",
    "\n",
    "xpath = (\n",
    "    f\"//div[@id='PageContent']\"\n",
    "    f\"//div[@id='JobResults']\"\n",
    "    f\"//p[@class='jobsCount']\"\n",
    "    )\n",
    "element = scr.get_element_when_present(browser=browser, xpath=xpath)\n",
    "\n",
    "n_jobs = int(element.text.split()[0])\n",
    "n_pages = math.ceil(346 / 30)\n",
    "\n",
    "print(\n",
    "    f\"\\tjobs {n_jobs}\\n\"\n",
    "    f\"\\tpages {n_pages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in range(n_pages-9):\n",
    "    \n",
    "    xpath = (\n",
    "        f\"//div[@id='PageContent']\"\n",
    "        f\"//div[@id='JobResults']\"\n",
    "        f\"//ul[@class='jlGrid hover']\"\n",
    "        f\"//li[@class='jl']\")\n",
    "#     Wait for all element to be located.\n",
    "    WebDriverWait(\n",
    "        browser, 10).until(\n",
    "            EC.visibility_of_all_elements_located(\n",
    "                (By.XPATH, xpath)))\n",
    "#     Get all elements.\n",
    "    jobs_on_page = browser.find_elements_by_xpath(xpath)\n",
    "    \n",
    "    for j in jobs_on_page[:5]:\n",
    "        j.click()\n",
    "        browser.implicitly_wait(5)\n",
    "        \n",
    "        name, rating = scr.get_company_name_and_rating(browser=browser)\n",
    "        browser.implicitly_wait(5)\n",
    "        \n",
    "        job_title, job_description = scr.get_job_title_and_description(browser=browser)\n",
    "        job_description_encoded = job_description.encode('utf-8')\n",
    "        job_description_hash = hashlib.sha224(job_description_encoded).hexdigest()\n",
    "\n",
    "        try:\n",
    "            size, url = scr.get_company_size_and_url(browser=browser)\n",
    "        except NoSuchElementException:\n",
    "            size = 'NA'\n",
    "            url = 'NA'\n",
    "            \n",
    "        scraped_results.append({\n",
    "            'search_date': str(search_date),\n",
    "            'company_name': name,\n",
    "            'company_rating': rating,\n",
    "            'company_size': size,\n",
    "            'company_url': url,\n",
    "            'job_title': job_title,\n",
    "            'job_description': job_description,\n",
    "            'job_hash': job_description_hash})\n",
    "        \n",
    "    scr.switch_to_next_page(browser=browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_file(\n",
    "    data,\n",
    "    file_path):\n",
    "    \n",
    "    with open(file_path, 'a+') as file:\n",
    "        file.write(f'{json.dumps(data)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in scraped_results:\n",
    "#     json_job = json.dumps(job)\n",
    "    append_to_file(job,\n",
    "    file_path='../test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_saved_jobs(\n",
    "    file_path):\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        lines = json_file.readlines()\n",
    "    jobs = []\n",
    "    for l in lines:\n",
    "        jobs.append(json.loads(l))\n",
    "    jobs_df = pd.DataFrame(jobs)\n",
    "    return jobs_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_miscellanea",
   "language": "python",
   "name": "venv_miscellanea"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
